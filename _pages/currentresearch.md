---
permalink: /current-research/
title: "Current Research"
excerpt: ""
author_profile: true
redirect_from: 
  - "/nmp/"
  - "/nmp.html"
---

Following are the areas that I am working on:

Fake News Detection
------
*Organization* - Netaji Subhas University of Technology <br />
*Collegues* - Ridam Srivastava <br />
*Current State* - Submitted, ESWA, Elsevier <br />

We propose a novel stacked ensemble based architecture for multimodal fake news detection by employing decision level fusion of modalities. To choose the image modality we do a deep test of all existing architectures while keeping a watch on the accuracy-parameter tradeoff. For the image modality, we propose the use of NASNet Mobile Architecture and we report parameter reduction by atleast 4 times while improving image-only accuracy by 7%.For text modality, we introduce an ensemble of two SOTA architectures - ELECTRA and BERT. The ensemble is merged at the decision level using equal weightage averaging. Image and text modalities are also merged at the decision level with the use of equal weightage averaging. The proposed model achieves SOTA performance on the benchmarks while reducing runtime by at least 3 times.


Text Summarization
------
*Organization* - Netaji Subhas University of Technology <br />
*Collegues* - Ridam Srivastava <br />
*Current State* - Minor Revision, Knowledge Based Systems, Elsevier <br />

![image](https://prabhav55221.github.io//images/tsum.png)

This work investigates an unsupervised extractive summarisation approach that combines topic modeling and clustering. For topic modeling, Latent Dirichlet Allocation is used while K-Medoids clustering is employed for summary generation. The approach is evaluated on two datasets - Wikihow and CNN/DailyMail. The Recall Oriented-Understudy for Gisting Evaluation
(ROUGE) metrics are used for comparative analysis against recently reported techniques, specifically ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L (R-L). The suggested framework
offers scores of 32.30% (R-1), 9.13% (R-2) and 34.80%(R-L) on Wikihow and 43.90% (R-1), 19.01%(R-2) , 41.50% (R-L) on CNN/DailyMail, and the reported performances are found to be
superior. As a result of these promising outcomes, it is concluded that an unsupervised extractive summarisation approach with greater subtopic focus significantly improves over generic topic modeling, semantic and deep learning approaches. 


AQG Systems
------
*Organization* - D.Kraft Singapore & IIIT Delhi <br />
*Collegues* - Prof. Debarka Sengupta <br />
*Current State* - Patent Under Review (IPO, Singapore) <br />

![image](https://prabhav55221.github.io//images/aqg.png)

In today’s post COVID scenario, it has become imperative to develop smart e-learning solutions to manage distance learning in large institutions. While multiple solutions have been presented to manage these education platforms and to connect the educator to the student, there remains a dearth of systems which can manage the automatic generation and correction of quizzes and assessments. The few existing systems are restrictive in their domain areas and do not exploit the full benefit of modern AI technologies. In this work, we present a solution to this problem by proposing a novel end-to-end approach for quiz generation and correction powered by a bi-modular deep learning based framework. The proposed system can generate and correct both subjective and objective questions thus easing the workload on an instructor. For objective questions, the system allows for absolute correction grading while for subjective questions, it proposed multiple answers to reduce bias towards a single answer. The proposed system is tested on multiple domain areas and robust performance is observed. We also propose certain use cases of the system.



Previous Work
======

These are the areas that I have worked on in the past.

Speech Emotion Recognition
------
*Organization* - Netaji Subhas University of Technology <br />
*Collegues* - Ridam Srivastava <br />
*Current State* - [Published](https://www.sciencedirect.com/science/article/abs/pii/S0950705121005785) <br />

![image](https://prabhav55221.github.io//images/ser.png)

Speech emotion recognition (SER) plays a crucial role in improving the quality of man–machine interfaces in various fields like distance learning, medical science, virtual assistants, and automated customer services. A deep learning-based hierarchical approach is proposed for both unimodal and multimodal SER systems in this work. Of these, the audio-based unimodal system proposes using a combination of 33 features, which include prosody, spectral, and voice quality-based audio features. Further, for the multimodal system, both the above-mentioned audio features and additional textual features are used. Embeddings from Language Models v2 (ELMo v2) is implemented to extract word and character embeddings which helped to capture the context-dependent aspects of emotion in text. The proposed models’ performances are evaluated on two audio-only unimodal datasets – SAVEE and RAVDESS, and one audio-text multimodal dataset – IEMOCAP. The proposed hierarchical models offered SER accuracies of 81.2%, 81.7%, and 74.5% on the RAVDESS, SAVEE, and IEMOCAP datasets, respectively. Further, these results are also benchmarked against recently reported techniques, and the reported performances are found to be superior. Therefore, based on the presented investigations, it is concluded that the application of a deep learning-based network in a hierarchical manner significantly improves SER over generic unimodal and multimodal systems.


<!-- Brain MRI Segmentation
------


TTR Estimation in Infants
------


COVID-19 Scenario in USA and India - A Topic Modelled Approach
------ -->

